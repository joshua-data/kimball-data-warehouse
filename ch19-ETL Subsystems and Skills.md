# 📘 학습 노트: ETL 서브시스템과 기술

## 1. ✅ ETL 시스템의 중요성과 복잡성

- ETL은 DW/BI 시스템 구축 시 **가장 많은 시간과 리소스가 투입**되는 핵심 영역.
- '추출(Extract), 변환(Transform), 적재(Load)'라는 간단한 단어와 달리, 실제 구현은 복잡한 요구 사항과 제약 조건 하에 이루어짐.
- 무계획적이고 임기응변식 접근은 **혼란과 비효율**을 초래함 → 반드시 **표준화된 서브시스템 아키텍처** 필요.
- **34개의 ETL 서브시스템**을 기반으로 성공적인 DW/BI 프로젝트가 구축됨 (킴벌 그룹 기준).

---

## 2. 🏗️ ETL 설계 전 요구 사항 수집

ETL 설계 시작 전, 아래 10가지 항목에 대해 답변하고 체크리스트를 작성해야 함.

요구 사항 수집은 **제약 사항 파악**과 **핵심 기능 명세화**의 과정임.

### 2.1 비즈니스 요건

- **KPI 정의** 및 정기적 관리 필요.
- 비즈니스 사용자의 **의사결정 지원**을 위한 정보 콘텐츠 제공이 핵심.
- KPI 변경 사유 추적 및 관련 항목 검토 가능해야 함.

### 2.2 규제 준수

- **법무팀/규제 책임자**와의 협업 필요.
- 보고서의 생성 흐름(원천→변환→리포트)을 **입증 가능하게 설계**해야 함.
- 보관해야 할 데이터 항목과 **보관 기간 정의** 필요.

### 2.3 데이터 품질

- 품질 확보가 중요한 3가지 이유:
    1. 데이터 기반 의사결정 문화
    2. 글로벌 분산 시스템 간 통합 필요
    3. 증가하는 규제 대응
- 데이터 프로파일링을 통해 품질 이슈 사전 탐지 및 ETL 내 **모니터링 체계 포함**.

### 2.4 보안

- DW는 **데이터의 광범위한 접근**을 허용하지만, 보안은 이를 제한함 → 충돌 지점 발생.
- **보안 정책 수립** 후 ETL에 반영 필요.
- 보안 전문가가 설계 초기부터 참여해야 함.

### 2.5 데이터 통합

- 완전한 통합을 위해 **마스터 데이터 관리(MDM)** 시스템이 전제되어야 함.
- 표준 디멘션: 공통된 속성(예: 고객, 상품)
- 표준 팩트: 공통된 측정 값(예: 매출, 주문수)

---

## 3. 🔧 ETL 서브시스템 개요 (총 34개, 킴벌 기준)

### 🟨 [1] 요구 사항 수집 관련

- 프로젝트 범위 정의, 핵심 KPI 관리, 데이터 소스 식별 등

### 🟧 [2] 데이터 추출 관련 (3개 서브시스템)

- 소스 시스템에서의 **데이터 추출 절차 자동화**
- 실시간/배치 처리 조건 정의
- **증분 추출 로직 설계**

### 🟥 [3] 데이터 품질 및 정제 관련 (5개 서브시스템)

- 오류 감지 및 알림
- 데이터 표준화, 코드 통합, 유효성 검사

### 🟩 [4] 디멘션 적재 관련 (13개 서브시스템)

- SCD 처리 (Type 1~7)
- 디멘션 키 생성 및 이력 관리
- 누락 및 중복 대응

### 🟦 [5] 운영 및 모니터링 관련 (13개 서브시스템)

- 로깅, 에러 트래킹
- ETL 스케줄링 및 재처리
- 경고 시스템, SLA 모니터링

---

## 1️⃣ 보관과 연결 추적성 (Preservation & Lineage)

### ✅ 핵심 개념

- **데이터는 반드시 보수적으로 보관**해야 함.
- **임시 저장 영역도 영구 보관 고려**: 미래 사용 가능성이 불명확하면 보관하는 것이 원칙.
- **ETL 재처리보다 과거 데이터를 재사용하는 것이 더 안전**.
- **메타데이터와 연결 추적(lineage)은 필수**: 규제 대응 및 감사(Audit) 목적으로도 필요.

### 📌 실무 체크리스트

- [ ]  ETL 경로상의 주요 단계마다 데이터를 임시 저장
- [ ]  사용 여부와 관계없이 임시 영역의 데이터를 영구 보관 고려
- [ ]  각 단계별 메타데이터 작성 및 관리
- [ ]  데이터 원천, 중간 처리 데이터 목록화
- [ ]  보관 정책, 개인정보, 보안 정책과 연계한 문서화

---

## 2️⃣ BI 애플리케이션 데이터 제공 인터페이스

### ✅ 핵심 개념

- **ETL의 최종 목표 = BI 애플리케이션에 알맞은 데이터 제공**
- **복잡하고 정규화된 구조는 BI에 비효율적** → 다차원 모델로 단순화 필요
- **모델링팀과 BI개발자, ETL팀의 협업 필수**

### 📌 실무 체크리스트

- [ ]  BI에서 사용하는 **팩트/디멘션 테이블 목록화**
- [ ]  **OLAP 큐브, 집계 테이블 구조** 문서화
- [ ]  BI 툴의 데이터 요구사항 정리
- [ ]  인덱스 및 성능 향상 구성을 고려한 설계
- [ ]  데이터는 **단순하고 빠르게 조회 가능하도록 구조화**

---

## 3️⃣ 가용한 요소 기술들 (Available Technologies)

### ✅ 핵심 개념

- **ETL 설계 결정은 기술 스택과 팀 역량에 맞춰야 함**
- **C++, Java 등의 로우레벨 언어는 팀 역량이 없다면 지양**
- **직접 개발 vs 상용 툴 선택은 장기적 유지보수 관점에서 고려**

### 📌 실무 체크리스트

- [ ]  현업에서 사용하는 운영체제, ETL 툴, 스크립트 언어, 프로그래밍 언어, SQL, DBMS 목록화
- [ ]  각 기술에 대한 장점/단점 파악
- [ ]  이상적인(선호하는) 미래 기술 스택 정의
- [ ]  현재 시스템이 요구하는 기술과 비교 및 분석

---

## 4️⃣ 기존 라이선스 고려 (Legacy License Constraints)

### ✅ 핵심 개념

- **조직의 기존 라이선스를 고려한 설계 필요**
- **기존 라이선스를 고집하는 결정은 정치적일 수 있음**
- **실무자 관점에서 리스크가 큰 경우, 준비된 설명과 대안 필요**

### 📌 실무 체크리스트

- [ ]  조직의 보유 라이선스 목록화: 운영체제, ETL 툴, 스크립트 언어, SQL, DBMS, OLAP 등
- [ ]  기존 도구 활용 시 발생할 수 있는 한계 정리
- [ ]  예외 허용 여부, 대체 가능한 옵션 함께 정리
- [ ]  상위 관리자와의 논의 시 사용할 근거 자료 준비

---

## 5️⃣ 34개의 ETL 서브시스템 개요

Kimball은 ETL 시스템을 **4가지 구성요소와 34개 서브시스템**으로 나눔.

### 🧱 구성 요소별 설명

### 📥 1. 추출 (Extraction)

- 목적: 원천 데이터를 ETL 환경으로 **그대로 추출**
- 관련 서브시스템: **1~3**

### 🧹 2. 정제와 표준화 (Cleaning & Conforming)

- 목적: 표준 디멘션/메트릭 생성, 품질 보정
- 관련 서브시스템: **4~8**

### 📤 3. 전달 (Delivery)

- 목적: 다차원 모델 구조에 **적재 및 구성**
- 관련 서브시스템: **9~21**

### ⚙️ 4. 관리 (Management)

- 목적: ETL 프로세스 전반을 **운영 및 모니터링**
- 관련 서브시스템: **22~34**

---

# 📘 ETL 아키텍처: 추출 단계 학습 노트

## 🔹 추출(Extraction)의 개요

- ETL 아키텍처의 시작점.
- 원천 데이터를 이해하고, 이를 ETL 시스템이 독립적으로 사용할 수 있도록 **데이터 웨어하우스로 옮기는 과정**.
- 이 과정은 단순히 데이터를 복사하는 것이 아니라, DW/BI 시스템이 신뢰할 수 있는 데이터로 전환하기 위한 첫 단계다.

---

## 🔸 서브시스템 1: **데이터 프로파일링 (Data Profiling)**

### 📌 정의

- 원천 데이터의 **내용, 일관성, 구조**를 기술적으로 분석하는 활동.
- SELECT DISTINCT 쿼리처럼 데이터 값을 조사하는 것도 포함되지만, **전문 툴**을 사용하는 것이 일반적이고 생산적이다.

### 🎯 목적과 활용

| 전략(Strategic) | 전술(Tactical) |
| --- | --- |
| - 데이터 원천이 유효한지 조기 판단- DW에 포함할지 여부 선결정 | - 세부 데이터 문제 파악- ETL 설계/개발에 직접 활용 |
- 전략적 판단은 **프로젝트 초반**에 수행되어야 한다.
- DW 프로젝트 중간에 데이터 품질 문제를 인지하는 경우, **심각한 실패**로 이어질 수 있음.

### 🧰 툴 사용 권장

- 수작업 프로파일링보다 시각적 인터페이스와 탐색이 용이한 **전문 툴** 사용 권장.
- 결과는 개발 일정 및 데이터 수집 전략 조정에도 활용.

---

## 🔸 서브시스템 2: **변경 데이터 캡처(Change Data Capture, CDC)**

### 📌 정의

- **가장 최근 적재 이후** 변경된 데이터만 추출하여 전송.
- 전체 데이터를 매번 가져오는 방식의 **비효율성을 해결**하기 위한 기법.

### 📌 CDC 시스템의 핵심 요건

- 일부 변경만 처리 가능하도록 변경 구분.
- 삭제/편집/추가 모두를 캡처.
- 오류와 정상 구분을 위한 **의미 있는 코드** 사용.
- **메타데이터 포함**하여 규제 준수 가능.
- 가능한 **초기에 변경 감지** 수행하여 성능 최적화.

---

## ✅ CDC 전략별 기법 정리

| 전략 | 설명 | 장점 | 단점 |
| --- | --- | --- | --- |
| **1. 감사 칼럼 (Audit Column)** | created_at, updated_at 같은 타임스탬프 필드 활용 | 비교적 간단, 널리 사용됨 | 삭제 감지 불가, 애플리케이션 외부 변경시 불안정 |
| **2. 시간 기준 추출** | 특정 시간 이후 변경된 데이터만 조회 (`SYSDATE-1`) | 구현 쉬움 | 중복/누락 가능성 큼, 오류 발생 시 수동 정리 필요 |
| **3. 전체 변경 비교** | 전일 스냅샷과 비교 | 변경 완전 추적 가능 | 성능 부담 큼, 많은 자원 소모 |
| **4. 로그 스크랩 (Log Scraping)** | Redo/Transaction 로그 분석 | 매우 정확함 | 복잡도 높고, DBA 협조 필요, 실패 시 전체 로그 손실 위험 |

---

## 💡 실무에서의 시사점

- **데이터 프로파일링은 필수 선행작업**이며, 전략과 전술적 판단에 모두 중요하다.
- **CDC는 각 원천 시스템의 특성과 성숙도에 따라 가장 적절한 전략**을 선택해야 한다.
- 감사 칼럼이 있어도 **변경 감지에 신뢰성이 낮다면 대체 방법**이 필요하다.
- CDC 구현이 어려울 경우, **ETL 설계 난이도**가 급격히 올라간다.

# 📘 데이터 정제와 표준화 학습 노트

---

## 1️⃣ 데이터 정제와 표준화의 중요성

### ✅ 핵심 개념

- ETL의 **핵심 가치는 데이터 정제 및 표준화**에 있다.
- 단순한 데이터 추출·적재와는 다르게, **데이터 정제/표준화는 비즈니스 가치 창출에 직결됨**.
- 잘 설계된 정제 서브시스템은 **지저분한 데이터의 원인을 파악**하고, **운영 시스템의 문제까지 진단**할 수 있게 함.

---

## 2️⃣ 데이터 품질 관리 문화의 필요성

### 🙅 흔한 오해

- “데이터가 잘못된 건 입력자의 실수다” → **기술적 접근만으로는 해결 어려움**.

### 📌 현실적인 문제 예시

- 주민번호 입력 칸에 무효한 번호(예: `999999-9999999`) 입력 → 시스템 통과를 위해 사용자가 **가짜 값 생성**.
- 잘못된 데이터 입력은 **사용자의 책임이 아니라 시스템의 구조적 문제**일 수 있음.

### 📖 Michael Hammer의 지적

> "작은 데이터 품질 문제는 망가진 비즈니스 프로세스의 중요한 지표다."
> 

---

## 3️⃣ 품질 문화를 위한 조직 차원의 접근

### 🧱 데이터 품질 향상을 위한 9가지 조직적 접근 템플릿

1. **상위 리더십의 명확한 위임**
2. **임원 주도의 프로세스 리엔지니어링**
3. **데이터 입력 환경에 대한 투자**
4. **애플리케이션 통합 수준 향상**
5. **프로세스 개선을 위한 투자**
6. **조직 내 데이터 품질 인식 제고**
7. **부서 간 협력 촉진**
8. **품질 향상에 대한 공식적 칭찬**
9. **지속적인 품질 측정 및 향상**

---

## 4️⃣ 서브시스템 4: 데이터 정제 시스템

### 🎯 목표

- 데이터 품질 이슈 **탐지 및 관리**
- **포괄적인 품질 관리 아키텍처** 제공
- ETL 오류를 미리 방지하거나 잘 대응할 수 있는 구조 확립

### 💡 데이터 정제를 위한 시작점

- **데이터 프로파일링부터 시작하라**
    - → 데이터의 패턴, 이상치, 분포 등을 파악
    - → 정제에 필요한 요소 판단

---

## 5️⃣ 데이터 품질 스크린 (Quality Screen)

ETL의 흐름 속에서 **테스트 및 진단 필터 역할 수행**.

### 🔧 처리 흐름

- 테스트 **성공** → 문제 없이 통과
- 테스트 **실패** → 오류 이벤트 기록 or 처리 중단 or 오류 표시

### 📂 스크린의 3가지 유형 (Jack Olson 분류)

| 스크린 유형 | 설명 | 예시 |
| --- | --- | --- |
| ✅ **칼럼 스크린** | 단일 컬럼의 값 검증 | NULL 여부, 포맷 체크, 값 범위 검사 등 |
| ✅ **구조 스크린** | 컬럼 간 구조적 관계 검증 | 외래키-기본키, 계층 구조, 주소 유효성 등 |
| ✅ **비즈니스 규칙 스크린** | 복잡한 비즈니스 로직 검증 | 고객 프로파일, 이상 패턴 탐지, 임계치 초과 등 |

---

## 6️⃣ 품질 이슈 대응

> 스크린 실패 시, 어떻게 대응할지 결정하는 전략이 중요하다.
> 
- 오류 발생 시 기록
- 데이터 제외 or 오류 표시 후 계속 진행
- 오류 로그를 통해 **문제 패턴 식별 및 개선**

---

# 📘 학습 노트: 데이터 변환/적재 – 프레젠테이션 계층을 위한 준비

## ✅ 1. 개요: 데이터 변환/적재의 중요성

- ETL의 중심 역할은 **다차원 모델** (팩트 + 디멘션 테이블) 로 데이터를 보내는 것.
- 다차원 모델은 신뢰성과 유지보수 효율성이 핵심 → **정의된 변환/적재 기법**이 매우 중요.
- 특히 **디멘션 테이블 중심의 변환/적재**에 초점이 맞춰짐.

📌 디멘션 테이블은 팩트 테이블보다 작지만, **팩트의 기준 축을 제공하는 핵심 역할**을 함.

---

## 🔁 2. 데이터 변환/적재 흐름 요약

1. 원천 시스템으로부터 데이터 수집 (정제 완료)
2. 디멘션 로우 생성 (기초적 변환 작업 포함)
    - 대체 키 할당
    - 코드 → 설명값 변환
    - 칼럼 분할/병합
    - 반정규화 디멘션 생성
3. 팩트 테이블 생성
    - 사용자가 보는 핵심 수치 담음
    - 크고 무거운 작업이지만 로직 자체는 비교적 단순

---

## 🧠 3. 서브시스템 9: 디멘션 이력 관리자

### 🎯 목적: 디멘션 속성 변경 이력을 추적하고 관리

- 변경 발생 시, **정확하고 일관되게 반영**되어야 함
- 대표적인 변경 대응 방식 3가지 (SCD – Slowly Changing Dimension):

| 유형 | 설명 | 용도 |
| --- | --- | --- |
| **Type 1** | 덮어쓰기 | 단순 수정, 이력 필요 없음 |
| **Type 2** | 신규 로우 추가 | 속성 변경 이력 필요 시 |
| **Type 3** | 신규 칼럼 추가 | 제한된 과거 정보 유지 시 |

---

## 🔄 4. Type 1: 덮어쓰기

### ✅ 특징

- 기존 디멘션 로우의 특정 칼럼 값을 **UPDATE**로 수정
- 변경 이력은 저장 ❌ → 현재값만 유지

### ✅ 사용 시기

- **정확한 현재 상태만 중요**할 때
- 과거 값에 대한 분석이 **불필요**할 때

### ⚠️ 주의 사항

- UPDATE는 **성능 저하 가능성** 있음 → INSERT와 구분해서 사용
- 변경 칼럼이 사용된 **집계 테이블은 무용지물** → 재생성 필요
- ETL 툴에서 UPDATE/INSERT 구분 및 벌크 로딩 고려

---

## 🆕 5. Type 2: 신규 로우 추가

### ✅ 특징

- **속성 변경 이력 추적**을 위해 새 로우 추가
- 새로운 **대체 키**로 새로운 버전의 디멘션 생성

### ✅ 처리 절차

1. 변경 감지 (변경 데이터 캡처: CDC 활용)
    - 예: CRC 비교나 타임스탬프 기반
2. 기존 디멘션 로우 복사
3. 새로운 속성 반영하여 **새 로우 삽입**
4. **대체 키 매핑 테이블** 갱신 (최신 대체 키 확인용)

### ✅ 필수 ETL 정리 칼럼

| 칼럼명 | 설명 |
| --- | --- |
| 변경 일자 | 언제 변경되었는지 |
| 로우 유효 일시 (시작/종료) | 유효한 기간 정의 |
| 변경 사유 | 선택 사항 |
| 현재 유효 플래그 | TRUE / FALSE |
| 기타 메타데이터 | 예: 최근 변경 일시 등 |

> 📌 타입 2에서는 집계 테이블을 과거 시점으로 소급하여 재생성하지 않음.
> 

---

## 📌 참고 사항

- 백엔드 스크립트가 DB에서 직접 값을 바꾼 경우, '변경 일시' 컬럼 갱신 누락 위험 → CDC 신뢰 불가
- 이럴 경우, ETL에서는 **현재 일자 또는 시스템 일자**로 일관된 기준을 설정하는 것이 바람직

## 🧩 서브시스템 13: 팩트 테이블 구성기 (Fact Table Builder)

팩트 테이블은 **측정값(측정된 숫자)**을 저장하며, **디멘션 테이블**들과 연결되어 다차원 분석이 가능하게 한다.

이 서브시스템은 **세 가지 유형의 팩트 테이블**을 효과적으로 구성하고 적재하기 위한 ETL 전략을 설명한다.

### 🔹 1. 트랜잭션 팩트 테이블 적재기

- **정의**: 가장 **세밀한 그레인**을 기준으로 이벤트를 기록하는 테이블
- **예시**: 송장 상세, 매장 스캐너 처리
- **특징**
    - 매우 세부적이고 대용량
    - **단순히 insert만 수행**, 기존 row 갱신 없음
    - **파티션 구성**이 중요 (일자 등 기준)
    - **감사 칼럼**(timestamp, seq ID 등)을 통해 적재 상태 추적
- **ETL 전략**
    - CDC(Change Data Capture) 시스템에서 수신
    - 새 row만 적재 (append-only)
    - 과거 row 수정 필요시 → “2단계 전략”: 추가 → 삭제

### 🔹 2. 주기적 스냅샷 팩트 테이블 적재기

- **정의**: 일정 주기마다 특정 상태나 합계를 저장
- **예시**: 월간 잔액, 재고 현황, 재무 리포트
- **특징**
    - 일반적으로 일, 주, 월 단위
    - 트랜잭션 테이블처럼 **append 방식** 적재
    - **롤링 스냅샷**을 통해 현재 상태 갱신도 가능
- **롤링 스냅샷 처리 팁**
    - 비즈니스 규칙 복잡 시 구현 난이도 ↑
    - 시스템 외 계산 의존 시 신중히 도입

### 🔹 3. 점진적 스냅샷 팩트 테이블 적재기

- **정의**: 하나의 row가 이벤트 발생에 따라 **점진적으로 업데이트**되는 구조
- **예시**: 주문 흐름(주문→선적→결제→반품)
- **특징**
    - 여러 일자 외래 키를 가짐 (주문일, 출하일, 결제일 등)
    - 하나의 row가 이벤트마다 **갱신됨 (update)**
    - 자주 update되는 특성상, **row 크기 증가**, 성능 저하 유의
- **ETL 전략**
    - 변화 발생 시마다 갱신 처리
    - 성능 유지 위해 **전체 삭제 후 재적재** 고려

### ✅ 실무 팁 요약

| 항목 | 트랜잭션 팩트 | 주기적 스냅샷 | 점진적 스냅샷 |
| --- | --- | --- | --- |
| 그레인 | 이벤트 단위 | 특정 시점 상태 | 프로세스 진행 상황 |
| 적재 방식 | insert-only | 주기적 insert | update |
| 예시 | 주문 상세, POS 로그 | 월간 잔액, 재고 현황 | 주문 전체 흐름 |
| 처리 난이도 | 쉬움 | 보통 | 어려움 |
| 파티셔닝 | 강력 추천 | 권장 | 경우에 따라 필요 |

---

## 🧩 서브시스템 14: 대체 키 파이프라인 (Surrogate Key Pipeline)

팩트 테이블은 각 디멘션 테이블의 **대체 키(surrogate key)**를 외래 키로 갖는다. 이 서브시스템은 **참조 무결성**을 보장하기 위한 ETL 프로세스를 설명한다.

### 🔹 핵심 개념

- **대체 키**: 디멘션 테이블에서 사용하는 내부 고유 식별자
- **원천 키**: 원천 시스템의 비즈니스 키 (예: 제품코드, 고객번호 등)
- **참조 무결성**: 팩트 테이블의 외래 키는 반드시 디멘션 테이블에 존재해야 함

### 🔹 문제 상황

1. **누락된 참조**
    - 팩트 테이블에 있는 제품 키 `323442`가 디멘션 테이블에 없음
    - 사용자 쿼리 시 결과 누락 발생 → 신뢰도 하락
2. **ETL에서 인지 못하고 진행될 위험**
    - 데이터 품질 문제로 이어짐

### 🔹 해결 전략

- **대체 키 매핑 처리 로직 구축**
    - 팩트 적재 전, 원천 키 → 대체 키로 변환
    - 변환 실패시: 예외 처리, 디폴트 키(`unknown`) 지정
- **키 충돌 해결**
    - 원천 키가 중복되거나 다르게 매핑될 경우, 로그 기록 및 에러 처리
- **ETL 파이프라인 내 포함**
    - 이 과정은 **팩트 테이블 적재 이전 단계**로 필수 삽입되어야 함

### ✅ 실무 팁

- 디폴트 키(`1` 또는 `0`) 정의: 존재하지 않는 디멘션 참조시 대체 사용
- “Unknown” 행을 디멘션 테이블에 반드시 포함시켜야 함
- 키 매핑 실패 로그 저장 → 주기적 점검 및 디멘션 테이블 보완 필요

## ✅ 서브시스템 17: 디멘션 관리자 (Dimension Manager)

### 💡 개념

- **표준 디멘션(Standard Dimensions)**을 **중앙에서 설계하고 관리**하는 핵심 책임자
- 모든 **팩트 공급자**에게 동일한 디멘션을 공급하여 일관성 유지

### 📌 주요 역할

1. **디멘션 정의 및 설명 정리**: 설계 시 모든 이해관계자의 합의된 정의 정리
2. **디멘션 신규 데이터 처리**:
    - 새로운 로우 추가
    - 새로운 대체 키 생성
3. **디멘션 이력 관리**:
    - **SCD Type 2**: 기존 데이터에 새 행과 대체 키 추가
    - **SCD Type 1 & 3**: 기존 행 값 변경 및 디멘션 버전 갱신
4. **디멘션 복제 및 배포**:
    - 모든 팩트 공급자에게 동시 배포 필요
    - 각 행에 **버전 번호**를 포함해 Drill Across 쿼리 지원

### ⚠️ 고려 사항

- 병렬 분산 환경(다중 DBMS, 다중 테이블스페이스 등)에서는 배포 난이도 ↑
- 동기화 및 일관된 배포를 위한 철저한 관리 필요

---

## ✅ 서브시스템 18: 팩트 공급자 (Fact Provider)

### 💡 개념

- **팩트 테이블**을 관리하는 주체로, **디멘션 관리자에게서 표준 디멘션을 수신**하여 사용

### 📌 주요 역할

1. **디멘션 수신 및 키맵 갱신**
    - 새로운 디멘션 로우 반영 및 **대체 키 파이프라인 갱신**
2. **팩트 테이블 데이터 적재**
    - 원천 키 → 대체 키 변환 후 로우 추가
3. **팩트 테이블 수정**
    - 스냅샷/지연/오류 반영
    - 쓸모없는 집계 제거 및 영향 받은 집계 재처리
4. **품질 관리 및 사용자 알림**
    - 모든 팩트 테이블 정확성 보장
    - 변경 사항 발생 시 사용자에게 알림

### ⚠️ 집계 재처리 조건

- **디멘션 버전 변경 안 됨** → 신규 데이터만 처리
- **디멘션 버전 변경됨** → 전체 집계 재계산 필요

---

## ✅ 서브시스템 19: 집계 생성자 (Aggregate Builder)

### 💡 개념

- **대용량 DW 성능 향상**을 위해 설계된 집계 테이블(aggregates)을 자동으로 생성 및 유지

### 📌 주요 역할

1. **집계 테이블 생성 및 관리**
    - 필요한 축소 디멘션 포함
    - **증분 갱신**이 기본이지만, 디멘션 변경 시 **전체 재생성** 필요
2. **ETL 시스템과의 연계**
    - 집계는 전통적인 ETL 프로세스에 포함됨
3. **성능 기반 집계 설계**
    - 사용자 불만, 쿼리 로그 등을 분석하여 성능 이슈 해결을 위한 집계 설계

### ⚠️ 팁

- 단순 합계(SUM) 중심의 팩트는 집계 생성에 유리
- 항상 **팩트 공급자**와 일관성 유지 필요

---

## ✅ 서브시스템 20: OLAP 큐브 생성자 (OLAP Cube Builder)

### 💡 개념

- **OLAP 큐브**는 다차원 분석 환경을 제공하여 분석가에게 피벗/드릴다운/스라이싱 등을 지원

### 📌 주요 역할

1. **OLAP 큐브의 기반**:
    - 관계형 DW의 **스타 스키마 기반**
    - 전통적인 ETL 처리 완료 후 큐브 적재
2. **계층 구조 검증 및 적재**
    - OLAP 툴은 계층 일관성에 매우 민감
    - 디멘션의 **계층 검증** 필수
3. **SCD 처리**
    - **SCD Type 2**: 새 멤버로 간주하여 OLAP에 적합
    - **SCD Type 1**: 덮어쓰기는 큐브 손상 가능성 → 적합하지 않음

### ⚠️ 주의

- SCD Type 1은 모든 큐브를 다시 계산해야 할 수도 있으므로 위험

---

## ✅ 서브시스템 21: 데이터 전달 관리자 (Data Delivery Manager)

### 💡 개념

- DW/BI의 **프레젠테이션 계층**으로부터 데이터를 **외부 시스템이나 분석툴로 전달**하는 역할

### 📌 주요 역할

1. **비즈니스 파트너 및 외부 조직 제공용 데이터 추출**
    - 정부/공공기관, 건강보험 등 특정 목적으로 데이터 제공
2. **분석 애플리케이션 및 마이닝 툴 지원**
    - DW 테이블을 직접 참조하지 못하는 분석 툴에 맞춰 별도 데이터 포맷으로 가공 및 제공
    - 마이닝 툴을 위한 사전 가공 필수

### ⚠️ 유의점

- 데이터 정합성, 보안, 전달 형식 등을 철저히 관리
- 프레젠테이션 계층에서만 추출해야 함

---

## 📌 마무리 요약

| 서브시스템 | 주요 역할 요약 | 핵심 키워드 |
| --- | --- | --- |
| 17. 디멘션 관리자 | 표준 디멘션 정의 및 일관된 배포 | 대체 키, 버전 관리, SCD |
| 18. 팩트 공급자 | 팩트 테이블 유지 및 디멘션 수용 | 키맵, 집계 재처리, 사용자 알림 |
| 19. 집계 생성자 | 성능 최적화를 위한 집계 테이블 관리 | 증분/전체 재계산, 로그 기반 분석 |
| 20. OLAP 큐브 생성자 | OLAP 분석용 큐브 생성 | SCD Type 2 적합, 계층 일관성 |
| 21. 데이터 전달 관리자 | 외부 시스템/툴로 데이터 전달 | 데이터 포맷, 마이닝 툴 지원 |

# 📚 학습 노트: ETL 환경 관리

## ✅ ETL 환경 관리의 목적

DW/BI 시스템은 **신뢰받는 분석 플랫폼**으로 자리잡기 위해, 단순한 모델링이나 BI 도구의 구성만으로는 부족합니다. 핵심은 다음 3가지 품질 목표를 **지속적으로 충족**시키는 것입니다:

| 목표 | 설명 |
| --- | --- |
| **신뢰성** | 데이터의 일관성과 정확성 보장 |
| **가용성** | SLA(Service Level Agreement) 충족, 확장성 있는 운영 유지 |
| **관리성** | 변화에 유연하게 대응 가능한 구조 구성 |

---

## 🧩 서브시스템 22: **잡(Job) 스케줄러**

### 🎯 역할

ETL 프로세스를 **자동화하고 통제**하며, 안정적인 실행을 지원하는 시스템

### 📌 필수 구성 요소

| 구성 요소 | 설명 |
| --- | --- |
| **잡 정의** | 각 ETL 단계의 흐름과 종속성 관계 정의예: 고객 테이블이 실패하면 매출 팩트도 생성되지 않도록 방지 |
| **잡 스케줄링** | 시간 기반 or 이벤트 기반 스케줄링 기능 제공예: 전일 매출 집계 완료 후 시작 등 |
| **메타데이터 수집** | 실행 시간, 소요 시간, 상태 등의 로깅모니터링을 위한 대시보드 구성 가능 |
| **로깅** | 각 단계의 로그를 DB나 텍스트로 저장오류 재처리 및 성능 최적화 분석에 활용 |
| **알림** | 오류 발생 시 자동 알림 시스템 필요 (ex. 이메일, Slack 등) |

### 📌 고려 사항

- 다양한 잡들 간의 **종속성 처리 및 실패 복구** 메커니즘 필요
- SQL 저장 프로시저, Airflow, Control-M, AWS Step Functions 등 다양한 도구 활용 가능
- 실시간 처리라면 Kafka 기반의 **event-driven** scheduler 고려

---

## 💾 서브시스템 23: **백업 체계**

### 🎯 목적

예기치 못한 시스템 장애로부터 **데이터를 보호**하고, 신속한 복구를 가능하게 함

### 🔐 백업(Backup) 요건

| 기능 | 설명 |
| --- | --- |
| **고성능** | 짧은 시간 내 백업 완료 필요 (ex. 실시간 파티션 백업 등) |
| **단순한 관리** | GUI 기반 백업 스케줄링, 로그 조회 등 관리자 친화적 인터페이스 |
| **자동화 작동** | 저장소 장치 제어, 스케줄링, 알림 기능 포함 |

> ✅ 백업은 보통 물리적 백업을 의미 (데이터베이스 이미지, 인덱스 등 포함)
> 

---

### 📦 보관 및 복구(Archiving & Recovery)

| 주제 | 설명 |
| --- | --- |
| **보관 전략** | 오래된 데이터는 디스크 공간 차지 및 쿼리 성능 저하 → 저비용 스토리지로 이동 |
| **복구 전략** | 삭제/오류 발생 시 백업으로부터 원상 복구 가능해야 함 |
| **법적 요구 대응** | 특정 기간 이상 데이터 보존이 필요할 수 있음 (감사, 법적 이슈 등) |

📌 최근 트렌드:

- 디스크 저장소 비용이 낮아지면서 **온라인 디스크 기반 보관**이 일반화
- 클라우드 환경에서는 Glacier, Google Archive Storage 등 활용 가능

---

## 🧠 핵심 정리

| 항목 | 요약 |
| --- | --- |
| **ETL 신뢰성 확보** | 정확한 로직 + 자동화된 오류 대응 |
| **스케줄링 체계 구축** | 의존성 인식, 이벤트 기반 처리, 로깅 및 알림 필수 |
| **백업과 복구 체계** | 하드웨어 고장, 실수 등 대비책 구축 및 자동화 |
| **보관 전략** | 데이터 활용도 vs 비용 고려 → 저비용 저장소로 이전 |

---

### ✅ 서브시스템 25: 버전 제어 시스템 (Version Control System)

### 📌 개념

ETL 파이프라인상의 **모든 로직과 메타데이터**를 ‘스냅샷’ 형태로 **보관 및 복구**하는 기능.

ETL 구성 요소들의 변경 이력을 관리하며, 문제 발생 시 특정 시점으로 되돌릴 수 있도록 한다.

### 💡 핵심 기능

- **Check-in / Check-out** 기능 제공
- 버전 간 **차이 비교(diff)** 지원
- **ETL 흐름 전체를 저장/복원** 가능
- 전체 시스템에 영향을 주는 **마스터 버전 번호** 관리
- 이전 버전의 **완전한 복원 가능성 확보**

### 🛠️ 실무 적용 팁

- Git 또는 ETL 도구 자체의 버전 관리 기능 활용
- 배포 전 ETL 코드와 메타데이터 백업 습관화

---

### ✅ 서브시스템 26: 버전 이관 시스템 (Version Migration System)

### 📌 개념

ETL 개발 후 **다음 단계 환경(테스트/운영 등)** 으로 **버전별로 안전하게 이관**하는 체계.

### 💡 핵심 기능

- **개발 → 테스트 → 운영** 단계로 안전하게 이동
- **마이그레이션 자동화** (CI/CD 또는 도구 내 기능 활용)
- **버전 제어 시스템과 연동**

### 🔐 테스트 포인트

- 스키마 변경, 컬럼 추가, 인덱스 변경
- 집계 로직, 데이터베이스 파라미터
- BI 리포트 및 보안 정책 변경 사항

### 🛠️ 실무 적용 팁

- 환경별 격리된 워크스페이스 운영 (dev/test/prod)
- dbt, Airflow, GitHub Actions 등과의 연동 고려

---

### ✅ 서브시스템 27: 워크플로우 모니터 (Workflow Monitor)

### 📌 개념

ETL 잡의 **상태/성능/이력**을 지속적으로 **모니터링**하여 **신뢰성 있는 데이터 적재**를 보장.

### 💡 수집 대상 메타데이터

- 잡 상태: 실행/지연/중지/완료
- 성능: 처리건수, 오류 요약, 실행시간, 이력
- 인프라 자원: CPU, 메모리, 디스크, DB 성능

### 🚨 병목 원인 Top 10

1. 비효율적 인덱싱
2. 잘못된 SQL 옵티마이저 선택
3. 메모리 부족
4. 정렬 문제
5. 느린 변환 로직
6. 과도한 I/O
7. 불필요한 쓰기 작업
8. 전체 삭제 후 재적재
9. 늦게 적용되는 필터
10. 병렬처리 미활용

### 🛠️ 실무 적용 팁

- Airflow, dbt Cloud, 데이터 도구의 로그 활용
- 성능 경고 임계치 설정 → Slack/Email 알림 연동

---

### ✅ 서브시스템 28: 정렬 처리 시스템 (Sorting Subsystem)

### 📌 개념

ETL 중 **특정 순서대로 데이터를 정렬**하여 집계 및 연결 작업의 **정확성과 효율성**을 보장.

### 💡 정렬 도구 예시

- ETL 도구 내의 **정렬 함수**
- DBMS: `ORDER BY`, 인덱스 활용
- 전문 정렬 유틸리티: 빠르고 멀티 정렬 가능

### 🛠️ 실무 적용 팁

- 정렬 기준 컬럼에 **인덱스**를 적용하여 성능 향상
- 고속 정렬이 필요한 경우 전용 정렬 유틸리티 검토

---

### ✅ 서브시스템 29: 선행/후행 흐름 분석기 (Lineage Analyzer)

### 📌 개념

ETL 파이프라인 내에서 특정 데이터가 **어디에서 왔고(선행)**, **어디로 이어지는지(후행)** 분석.

### 📈 용도

- **선행 분석**: BI 리포트의 값이 어디서 유래되었는가?
- **후행 분석**: 원천 데이터 변경이 어떤 리포트에 영향을 주는가?

### 🧩 필수 요소

- 변환 로직 추적
- 중간 테이블 흐름 확인
- 리포트까지 연결된 데이터 경로 파악

### 🛠️ 실무 적용 팁

- dbt의 **lineage graph** 또는 **OpenLineage, DataHub** 등의 메타데이터 도구 도입

---

### ✅ 서브시스템 30: 오류 통지 시스템 (Error Notification System)

> ⚠️ 아직 내용이 누락되어 있거나 끝나지 않았습니다. 이후 내용에서 이어지는 부분을 기다리거나, 해당 부분을 다시 요청해주시면 정리 도와드리겠습니다.
> 

---

## ✨ 정리

| 서브시스템 | 핵심 키워드 | 목적 |
| --- | --- | --- |
| 25 | 버전 관리 | 복구, 추적 |
| 26 | 버전 이관 | 환경 전환 |
| 27 | 워크플로우 모니터링 | 성능 감시 |
| 28 | 정렬 처리 | 정확한 집계 |
| 29 | 데이터 라인리지 | 흐름 추적 |
| 30 | 오류 통지 | 안정 운영 (내용 미완성) |